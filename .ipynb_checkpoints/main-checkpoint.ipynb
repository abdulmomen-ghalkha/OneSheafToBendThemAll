{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d2005a-4669-4a3b-aab3-3f880458d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====== 1. Multi-modal MNIST dataset ======\n",
    "class MultiModalMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1, m2, m3), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MultiModalMNIST(train=True),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# ====== 2. Tiny CNN encoders per modality ======\n",
    "class TinyEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(16*7*7, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 3 nodes (modalities)\n",
    "num_modalities = 3\n",
    "embed_dim = 128\n",
    "encoders = nn.ModuleList([TinyEncoder(embed_dim).to(device) for _ in range(num_modalities)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ebaf529-f914-4da5-8ae3-45c2c1a09096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 3. Restriction maps P_{i->e} ======\n",
    "edges = [(0,1),(0,2),(1,2)]\n",
    "edge_dim = 64  # shared comparison space\n",
    "restrictions = nn.ParameterDict()\n",
    "for (i,j) in edges:\n",
    "    restrictions[f\"{i}->{i}-{j}\"] = nn.Parameter(torch.randn(edge_dim, embed_dim) * 0.1)\n",
    "    restrictions[f\"{j}->{i}-{j}\"] = nn.Parameter(torch.randn(edge_dim, embed_dim) * 0.1)\n",
    "\n",
    "# ====== 4. Loss functions ======\n",
    "def cosine_sim(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(1), b.unsqueeze(0), dim=-1)\n",
    "\n",
    "def contrastive_loss(p_i, p_j, tau=0.1):\n",
    "    # p_i, p_j: [B, D]\n",
    "    sim_ij = cosine_sim(p_i, p_j) / tau\n",
    "    labels = torch.arange(p_i.size(0)).to(device)\n",
    "    # Symmetric InfoNCE\n",
    "    loss_i = F.cross_entropy(sim_ij, labels)\n",
    "    loss_j = F.cross_entropy(sim_ij.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_j)\n",
    "\n",
    "def laplacian_loss(p_i, p_j):\n",
    "    return ((p_i - p_j)**2).sum(dim=1).mean()\n",
    "\n",
    "# ====== 5. Training loop ======\n",
    "optimizer = torch.optim.Adam(list(encoders.parameters()) + list(restrictions.parameters()), lr=1e-4)\n",
    "lambda_lap = 1.0\n",
    "beta_contrast = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c494a48-28d2-48f1-8506-362ccc8e47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2781\n",
      "Epoch 2, Loss: 0.1960\n",
      "Epoch 3, Loss: 0.1265\n",
      "Epoch 4, Loss: 0.1224\n",
      "Epoch 5, Loss: 0.1026\n",
      "Epoch 6, Loss: 0.0593\n",
      "Epoch 7, Loss: 0.0816\n",
      "Epoch 8, Loss: 0.0674\n",
      "Epoch 9, Loss: 0.0497\n",
      "Epoch 10, Loss: 0.0517\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # small demo\n",
    "    for (mods, labels) in train_loader:\n",
    "        mods = [m.to(device) for m in mods]\n",
    "        batch_size = mods[0].size(0)\n",
    "\n",
    "        # Local embeddings h_i\n",
    "        h = [enc(mods[i]) for i, enc in enumerate(encoders)]  # list of [B, embed_dim]\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Loop over edges for sheaf contrastive + Laplacian loss\n",
    "        for (i,j) in edges:\n",
    "            P_i = restrictions[f\"{i}->{i}-{j}\"]\n",
    "            P_j = restrictions[f\"{j}->{i}-{j}\"]\n",
    "\n",
    "            p_i = (h[i] @ P_i.t())   # [B, edge_dim]\n",
    "            p_j = (h[j] @ P_j.t())   # [B, edge_dim]\n",
    "\n",
    "            lap_loss = laplacian_loss(p_i, p_j)\n",
    "            contrast_loss = contrastive_loss(p_i, p_j)\n",
    "\n",
    "            total_loss += lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ba7b80-1594-4d31-8966-ceecdac0b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Zero-Shot Eval] Using modality 0 embeddings\n",
      "Zero-Shot Accuracy (modality 0): 81.67%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 0 -> Gallery: 1\n",
      "Recall@1: 11.37%\n",
      "Recall@5: 28.67%\n",
      "Recall@10: 39.94%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 1 -> Gallery: 0\n",
      "Recall@1: 11.01%\n",
      "Recall@5: 28.93%\n",
      "Recall@10: 40.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "\n",
    "# ====== 0. Load your trained components ======\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# These should be the trained encoders and restriction maps from your training script\n",
    "#from train_multimodal_mnist import encoders, restrictions, edges  # <-- import your trained model parts\n",
    "embed_dim = 32\n",
    "edge_dim = 16\n",
    "\n",
    "# ====== 1. Multi-modal MNIST test dataset ======\n",
    "class MultiModalMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=False):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        m1 = TF.to_tensor(img)\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        return (m1, m2, m3), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "test_loader = DataLoader(MultiModalMNIST(train=False), batch_size=256, shuffle=False)\n",
    "\n",
    "# Helper to normalize embeddings\n",
    "def encode_modality(modality_idx, loader):\n",
    "    enc = encoders[modality_idx].eval()\n",
    "    embs, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for mods, y in loader:\n",
    "            x = mods[modality_idx].to(device)\n",
    "            h = enc(x)\n",
    "            h = F.normalize(h, dim=-1)\n",
    "            embs.append(h)\n",
    "            labels.append(y)\n",
    "    return torch.cat(embs), torch.cat(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc79e4fe-6f30-4cfc-9780-8e5a45dcbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Zero-Shot Eval] Using modality 0 embeddings\n",
      "Zero-Shot Accuracy (modality 0): 81.67%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2\n",
      "Recall@1: 9.92%\n",
      "Recall@5: 24.89%\n",
      "Recall@10: 34.46%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 2 -> Gallery: 0\n",
      "Recall@1: 9.61%\n",
      "Recall@5: 25.87%\n",
      "Recall@10: 36.45%\n"
     ]
    }
   ],
   "source": [
    "# ====== 2. Zero-Shot Learning Evaluation ======\n",
    "def zero_shot_eval(mod_idx=0):\n",
    "    \"\"\"Use the embedding space of modality `mod_idx` for classification via nearest centroid.\"\"\"\n",
    "    print(f\"\\n[Zero-Shot Eval] Using modality {mod_idx} embeddings\")\n",
    "    # Encode all test data\n",
    "    test_embs, test_labels = encode_modality(mod_idx, test_loader)\n",
    "\n",
    "    # Compute class centroids\n",
    "    num_classes = 10\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        class_embs = test_embs[test_labels == c]\n",
    "        centroids.append(class_embs.mean(dim=0))\n",
    "    centroids = F.normalize(torch.stack(centroids), dim=-1)  # [10, D]\n",
    "\n",
    "    # Classify by cosine similarity\n",
    "    sims = test_embs @ centroids.t()  # [N, 10]\n",
    "    preds = sims.argmax(dim=1)\n",
    "    acc = (preds == test_labels.to(device)).float().mean().item()\n",
    "    print(f\"Zero-Shot Accuracy (modality {mod_idx}): {acc*100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# ====== 3. Cross-Modal Retrieval ======\n",
    "def cross_modal_retrieval(query_mod=0, gallery_mod=1, top_k=(1,5,10)):\n",
    "    print(f\"\\n[Cross-Modal Retrieval] Query: {query_mod} -> Gallery: {gallery_mod}\")\n",
    "    q_embs, q_labels = encode_modality(query_mod, test_loader)\n",
    "    g_embs, g_labels = encode_modality(gallery_mod, test_loader)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim = q_embs @ g_embs.t()\n",
    "    ranks = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "    recalls = {k: 0 for k in top_k}\n",
    "    for i, label in enumerate(q_labels):\n",
    "        retrieved_labels = g_labels[ranks[i]]\n",
    "        for k in top_k:\n",
    "            if label in retrieved_labels[:k]:d\n",
    "                recalls[k] += 1\n",
    "\n",
    "    for k in top_k:\n",
    "        recalls[k] /= len(q_labels)\n",
    "        print(f\"Recall@{k}: {recalls[k]*100:.2f}%\")\n",
    "    return recalls\n",
    "\n",
    "# ====== 4. Run evaluations ======\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: evaluate ZSL for modality 0\n",
    "    zero_shot_eval(mod_idx=0)\n",
    "\n",
    "    # Example: cross-modal retrieval (0 -> 1 and 1 -> 0)\n",
    "    cross_modal_retrieval(query_mod=0, gallery_mod=2)\n",
    "    cross_modal_retrieval(query_mod=2, gallery_mod=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729afc6e-8083-445c-a64e-b9eaf1d39c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
