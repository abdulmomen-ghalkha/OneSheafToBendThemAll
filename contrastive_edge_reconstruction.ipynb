{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db40bacb-512c-43f5-b77e-07cd62e22531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.7384\n",
      "Epoch 2, Loss: 1.1660\n",
      "Epoch 3, Loss: 1.2381\n",
      "Epoch 4, Loss: 0.8045\n",
      "Epoch 5, Loss: 0.9656\n",
      "Epoch 6, Loss: 0.9429\n",
      "Epoch 7, Loss: 0.4589\n",
      "Epoch 8, Loss: 0.6917\n",
      "Epoch 9, Loss: 0.5351\n",
      "Epoch 10, Loss: 0.5625\n",
      "Epoch 11, Loss: 0.4575\n",
      "Epoch 12, Loss: 0.6223\n",
      "Epoch 13, Loss: 0.4560\n",
      "Epoch 14, Loss: 0.5552\n",
      "Epoch 15, Loss: 0.3097\n",
      "Epoch 16, Loss: 0.2188\n",
      "Epoch 17, Loss: 0.2892\n",
      "Epoch 18, Loss: 0.3468\n",
      "Epoch 19, Loss: 0.2762\n",
      "Epoch 20, Loss: 0.2042\n",
      "Epoch 21, Loss: 0.3284\n",
      "Epoch 22, Loss: 0.5363\n",
      "Epoch 23, Loss: 0.2427\n",
      "Epoch 24, Loss: 0.2566\n",
      "Epoch 25, Loss: 0.2504\n",
      "Epoch 26, Loss: 0.2343\n",
      "Epoch 27, Loss: 0.2034\n",
      "Epoch 28, Loss: 0.1608\n",
      "Epoch 29, Loss: 0.1951\n",
      "Epoch 30, Loss: 0.2055\n",
      "Epoch 31, Loss: 0.2039\n",
      "Epoch 32, Loss: 0.1841\n",
      "Epoch 33, Loss: 0.2830\n",
      "Epoch 34, Loss: 0.1670\n",
      "Epoch 35, Loss: 0.1931\n",
      "Epoch 36, Loss: 0.1272\n",
      "Epoch 37, Loss: 0.5061\n",
      "Epoch 38, Loss: 0.1659\n",
      "Epoch 39, Loss: 0.1814\n",
      "Epoch 40, Loss: 0.2031\n",
      "Epoch 41, Loss: 0.2190\n",
      "Epoch 42, Loss: 0.2357\n",
      "Epoch 43, Loss: 0.1466\n",
      "Epoch 44, Loss: 0.1377\n",
      "Epoch 45, Loss: 0.1942\n",
      "Epoch 46, Loss: 0.1666\n",
      "Epoch 47, Loss: 0.2239\n",
      "Epoch 48, Loss: 0.1625\n",
      "Epoch 49, Loss: 0.1252\n",
      "Epoch 50, Loss: 0.2243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====== 1. Multi-modal MNIST dataset ======\n",
    "class MultiModalMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1, m2, m3), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MultiModalMNIST(train=True),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# ====== 2. Tiny CNN encoders per modality ======\n",
    "class TinyEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(16*7*7, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 3 nodes (modalities)\n",
    "num_modalities = 3\n",
    "embed_dim = 128\n",
    "encoders = nn.ModuleList([TinyEncoder(embed_dim).to(device) for _ in range(num_modalities)])\n",
    "\n",
    "\n",
    "# ====== 3. Restriction maps P_{i->e} and Dual maps Q_{e->i} ======\n",
    "edges = [(0,1),(0,2),(1,2)]\n",
    "edge_dim = 64  # shared comparison space\n",
    "\n",
    "restrictions = nn.ParameterDict()  # P maps\n",
    "duals = nn.ParameterDict()         # Q maps\n",
    "\n",
    "for (i,j) in edges:\n",
    "    # P: node -> edge\n",
    "    restrictions[f\"{i}->{i}-{j}\"] = nn.Parameter(torch.randn(edge_dim, embed_dim) * 0.1)\n",
    "    restrictions[f\"{j}->{i}-{j}\"] = nn.Parameter(torch.randn(edge_dim, embed_dim) * 0.1)\n",
    "\n",
    "    # Q: edge -> node\n",
    "    duals[f\"{i}-{j}->{i}\"] = nn.Parameter(torch.randn(embed_dim, edge_dim) * 0.1)\n",
    "    duals[f\"{i}-{j}->{j}\"] = nn.Parameter(torch.randn(embed_dim, edge_dim) * 0.1)\n",
    "\n",
    "\n",
    "# ====== 4. Loss functions ======\n",
    "def cosine_sim(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(1), b.unsqueeze(0), dim=-1)\n",
    "\n",
    "def contrastive_loss(p_i, p_j, tau=0.1):\n",
    "    # p_i, p_j: [B, D]\n",
    "    sim_ij = cosine_sim(p_i, p_j) / tau\n",
    "    labels = torch.arange(p_i.size(0)).to(device)\n",
    "    # Symmetric InfoNCE\n",
    "    loss_i = F.cross_entropy(sim_ij, labels)\n",
    "    loss_j = F.cross_entropy(sim_ij.t(), labels)\n",
    "    return 0.5 * (loss_i + loss_j)\n",
    "\n",
    "def laplacian_loss(p_i, p_j):\n",
    "    return ((p_i - p_j)**2).sum(dim=1).mean()\n",
    "\n",
    "def reconstruction_loss(h_i, p_i, Q_ei):\n",
    "    # Reconstruct node embedding from edge embedding\n",
    "    # p_i: node->edge embedding [B, edge_dim]\n",
    "    # Q_ei: edge->node map [embed_dim, edge_dim]\n",
    "    recon = p_i @ Q_ei.t()  # [B, embed_dim]\n",
    "    return F.mse_loss(recon, h_i)\n",
    "\n",
    "# ====== 5. Training loop ======\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoders.parameters()) + list(restrictions.parameters()) + list(duals.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "lambda_lap = 1.0\n",
    "beta_contrast = 10.0\n",
    "gamma_recon = 0.1  # weight for reconstruction loss\n",
    "\n",
    "for epoch in range(50):  # small demo\n",
    "    for (mods, labels) in train_loader:\n",
    "        mods = [m.to(device) for m in mods]\n",
    "        batch_size = mods[0].size(0)\n",
    "\n",
    "        # Local embeddings h_i\n",
    "        h = [enc(mods[i]) for i, enc in enumerate(encoders)]  # list of [B, embed_dim]\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Loop over edges for sheaf contrastive + Laplacian + Reconstruction\n",
    "        for (i,j) in edges:\n",
    "            P_i = restrictions[f\"{i}->{i}-{j}\"]\n",
    "            P_j = restrictions[f\"{j}->{i}-{j}\"]\n",
    "            Q_ei = duals[f\"{i}-{j}->{i}\"]\n",
    "            Q_ej = duals[f\"{i}-{j}->{j}\"]\n",
    "\n",
    "            p_i = (h[i] @ P_i.t())   # [B, edge_dim]\n",
    "            p_j = (h[j] @ P_j.t())   # [B, edge_dim]\n",
    "\n",
    "            lap_loss = laplacian_loss(p_i, p_j)\n",
    "            contrast_loss = contrastive_loss(p_i, p_j)\n",
    "\n",
    "            # Reconstruction from edge back to nodes\n",
    "            recon_loss_i = reconstruction_loss(h[i], p_i, Q_ei)\n",
    "            recon_loss_j = reconstruction_loss(h[j], p_j, Q_ej)\n",
    "\n",
    "            \n",
    "            total_loss += (\n",
    "                lambda_lap * lap_loss +\n",
    "                beta_contrast * contrast_loss +\n",
    "                gamma_recon * (recon_loss_i + recon_loss_j)\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a36e5f-5cff-4112-b7b4-c3ad4f9e6b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Zero-Shot Eval] Using modality 0 embeddings (raw)\n",
      "Zero-Shot Accuracy (modality 0): 72.86%\n",
      "\n",
      "[Zero-Shot Eval] Using modality 0 embeddings (reconstructed)\n",
      "Zero-Shot Accuracy (modality 0): 57.18%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (raw)\n",
      "Recall@1: 10.20%\n",
      "Recall@5: 30.55%\n",
      "Recall@10: 44.53%\n",
      "\n",
      "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (reconstructed)\n",
      "Recall@1: 10.50%\n",
      "Recall@5: 31.18%\n",
      "Recall@10: 46.85%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====== 0. Load your trained components ======\n",
    "# encoders, restrictions, duals, edges should be loaded from your training\n",
    "# Example:\n",
    "# from train_multimodal_mnist import encoders, restrictions, duals, edges\n",
    "embed_dim = 128\n",
    "edge_dim = 64\n",
    "\n",
    "# ====== 1. Multi-modal MNIST test dataset ======\n",
    "class MultiModalMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=False):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        m1 = TF.to_tensor(img)\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        return (m1, m2, m3), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "test_loader = DataLoader(MultiModalMNIST(train=False), batch_size=256, shuffle=False)\n",
    "\n",
    "# ====== 2. Encode + Optionally Reconstruct via Dual Maps ======\n",
    "def encode_modality(modality_idx, loader, use_reconstruction=False):\n",
    "    enc = encoders[modality_idx].eval()\n",
    "    embs, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for mods, y in loader:\n",
    "            x = mods[modality_idx].to(device)\n",
    "            h = enc(x)  # [B, embed_dim]\n",
    "\n",
    "            if use_reconstruction:\n",
    "                # Aggregate reconstructions from all edges touching this node\n",
    "                reconstructions = []\n",
    "                for (i, j) in edges:\n",
    "                    if modality_idx in (i, j):\n",
    "                        edge_key = f\"{i}-{j}\"\n",
    "                        node_key = f\"{modality_idx}->{edge_key}\"\n",
    "                        dual_key = f\"{edge_key}->{modality_idx}\"\n",
    "                        P = restrictions[node_key]    # [edge_dim, embed_dim]\n",
    "                        Q = duals[dual_key]           # [embed_dim, edge_dim]\n",
    "\n",
    "                        edge_emb = h @ P.t()          # project to edge\n",
    "                        node_recon = edge_emb @ Q.t() # reconstruct node\n",
    "                        reconstructions.append(node_recon)\n",
    "\n",
    "                if len(reconstructions) > 0:\n",
    "                    h = sum(reconstructions) / len(reconstructions)\n",
    "\n",
    "            h = F.normalize(h, dim=-1)\n",
    "            embs.append(h)\n",
    "            labels.append(y)\n",
    "    return torch.cat(embs), torch.cat(labels)\n",
    "\n",
    "\n",
    "# ====== 3. Zero-Shot Learning Evaluation ======\n",
    "def zero_shot_eval(mod_idx=0, use_reconstruction=False):\n",
    "    print(f\"\\n[Zero-Shot Eval] Using modality {mod_idx} embeddings \"\n",
    "          f\"{'(reconstructed)' if use_reconstruction else '(raw)'}\")\n",
    "    # Encode all test data\n",
    "    test_embs, test_labels = encode_modality(mod_idx, test_loader, use_reconstruction)\n",
    "\n",
    "    # Compute class centroids\n",
    "    num_classes = 10\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        class_embs = test_embs[test_labels == c]\n",
    "        centroids.append(class_embs.mean(dim=0))\n",
    "    centroids = F.normalize(torch.stack(centroids), dim=-1)  # [10, D]\n",
    "\n",
    "    # Classify by cosine similarity\n",
    "    sims = test_embs @ centroids.t()  # [N, 10]\n",
    "    preds = sims.argmax(dim=1)\n",
    "    acc = (preds == test_labels.to(device)).float().mean().item()\n",
    "    print(f\"Zero-Shot Accuracy (modality {mod_idx}): {acc*100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ====== 4. Cross-Modal Retrieval ======\n",
    "def cross_modal_retrieval(query_mod=0, gallery_mod=1, top_k=(1,5,10), use_reconstruction=False):\n",
    "    print(f\"\\n[Cross-Modal Retrieval] Query: {query_mod} -> Gallery: {gallery_mod} \"\n",
    "          f\"{'(reconstructed)' if use_reconstruction else '(raw)'}\")\n",
    "\n",
    "    q_embs, q_labels = encode_modality(query_mod, test_loader, use_reconstruction)\n",
    "    g_embs, g_labels = encode_modality(gallery_mod, test_loader, use_reconstruction)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim = q_embs @ g_embs.t()\n",
    "    ranks = sim.argsort(dim=1, descending=True)\n",
    "\n",
    "    recalls = {k: 0 for k in top_k}\n",
    "    for i, label in enumerate(q_labels):\n",
    "        retrieved_labels = g_labels[ranks[i]]\n",
    "        for k in top_k:\n",
    "            if label in retrieved_labels[:k]:\n",
    "                recalls[k] += 1\n",
    "\n",
    "    for k in top_k:\n",
    "        recalls[k] /= len(q_labels)\n",
    "        print(f\"Recall@{k}: {recalls[k]*100:.2f}%\")\n",
    "    return recalls\n",
    "\n",
    "\n",
    "# ====== 5. Run evaluations ======\n",
    "if __name__ == \"__main__\":\n",
    "    # Zero-shot on raw and reconstructed embeddings\n",
    "    zero_shot_eval(mod_idx=0, use_reconstruction=False)\n",
    "    zero_shot_eval(mod_idx=0, use_reconstruction=True)\n",
    "\n",
    "    # Cross-modal retrieval\n",
    "    cross_modal_retrieval(query_mod=0, gallery_mod=2, use_reconstruction=False)\n",
    "    cross_modal_retrieval(query_mod=0, gallery_mod=2, use_reconstruction=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729afc6e-8083-445c-a64e-b9eaf1d39c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Zero-Shot Eval] Using modality 0 embeddings (raw)\\nZero-Shot Accuracy (modality 0): 75.59%\\n\\n[Zero-Shot Eval] Using modality 0 embeddings (reconstructed)\\nZero-Shot Accuracy (modality 0): 54.00%\\n\\n[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (raw)\\nRecall@1: 8.23%\\nRecall@5: 25.64%\\nRecall@10: 38.85%\\n\\n[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (reconstructed)\\nRecall@1: 9.04%\\nRecall@5: 28.85%\\nRecall@10: 43.70%\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings 128 \n",
    "'''\n",
    "[Zero-Shot Eval] Using modality 0 embeddings (raw)\n",
    "Zero-Shot Accuracy (modality 0): 67.30%\n",
    "\n",
    "[Zero-Shot Eval] Using modality 0 embeddings (reconstructed)\n",
    "Zero-Shot Accuracy (modality 0): 64.59%\n",
    "\n",
    "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (raw)\n",
    "Recall@1: 11.04%\n",
    "Recall@5: 31.41%\n",
    "Recall@10: 45.51%\n",
    "\n",
    "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (reconstructed)\n",
    "Recall@1: 10.28%\n",
    "Recall@5: 30.39%\n",
    "Recall@10: 44.75%\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Embeddings 64\n",
    "\n",
    "'''\n",
    "[Zero-Shot Eval] Using modality 0 embeddings (raw)\n",
    "Zero-Shot Accuracy (modality 0): 75.59%\n",
    "\n",
    "[Zero-Shot Eval] Using modality 0 embeddings (reconstructed)\n",
    "Zero-Shot Accuracy (modality 0): 54.00%\n",
    "\n",
    "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (raw)\n",
    "Recall@1: 8.23%\n",
    "Recall@5: 25.64%\n",
    "Recall@10: 38.85%\n",
    "\n",
    "[Cross-Modal Retrieval] Query: 0 -> Gallery: 2 (reconstructed)\n",
    "Recall@1: 9.04%\n",
    "Recall@5: 28.85%\n",
    "Recall@10: 43.70%\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c40784-7b3f-4ea2-bbef-f3614d1ffe23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
