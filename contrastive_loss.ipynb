{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1b9380-dd5f-4e8f-a7d7-e7db7c3b9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and graph setup complete.\n",
      "Graph: {0: [1, 2], 1: [0, 2], 2: [0, 1]}\n",
      "DataLoaders: 3 nodes ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "import random\n",
    "\n",
    "# ==== 1. Multi-modal MNIST Dataset ====\n",
    "class MultiModalMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1.view(-1), m2.view(-1), m3.view(-1)), label  # flatten each modality\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# --- Build training and testing sets ---\n",
    "train_dataset = MultiModalMNIST(train=True)\n",
    "test_dataset = MultiModalMNIST(train=False)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ==== 2. Graph Setup ====\n",
    "# We have 3 nodes (modality-specific learners)\n",
    "num_nodes = 3\n",
    "graph = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 2],\n",
    "    2: [0, 1]\n",
    "}\n",
    "\n",
    "# Node-specific DataLoaders (extracting single modality)\n",
    "def modality_dataloader(dataset, modality_idx):\n",
    "    class SingleModality(Dataset):\n",
    "        def __init__(self, base_dataset, idx):\n",
    "            self.base = base_dataset\n",
    "            self.idx = idx\n",
    "        def __len__(self):\n",
    "            return len(self.base)\n",
    "        def __getitem__(self, i):\n",
    "            Xs, y = self.base[i]\n",
    "            return Xs[self.idx], y\n",
    "    return DataLoader(SingleModality(dataset, modality_idx), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_loaders = {i: modality_dataloader(train_dataset, i) for i in range(num_nodes)}\n",
    "\n",
    "# ==== 3. Node Encoders and Maps ====\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Node Encoder ---\n",
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# --- Maps ---\n",
    "class RestrictionMap(nn.Module):  # Node -> Edge\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(node_dim, edge_dim, bias=False)\n",
    "    def forward(self, h):\n",
    "        return self.map(h)\n",
    "\n",
    "class TransportMap(nn.Module):  # Edge -> Node\n",
    "    def __init__(self, edge_dim, node_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(edge_dim, node_dim, bias=False)\n",
    "    def forward(self, z):\n",
    "        return self.map(z)\n",
    "\n",
    "# --- Initialize models ---\n",
    "input_dim = 28*28      # flattened MNIST\n",
    "embedding_dim = 64\n",
    "edge_dim = 32\n",
    "\n",
    "encoders = {i: NodeEncoder(input_dim, embedding_dim) for i in range(num_nodes)}\n",
    "\n",
    "# Initialize P and Q maps\n",
    "P_maps = {i: {} for i in range(num_nodes)}\n",
    "Q_maps = {i: {} for i in range(num_nodes)}\n",
    "for i in range(num_nodes):\n",
    "    for j in graph[i]:\n",
    "        P_maps[i][j] = RestrictionMap(embedding_dim, edge_dim)\n",
    "        Q_maps[i][j] = TransportMap(edge_dim, embedding_dim)\n",
    "\n",
    "# ==== 4. Optimizers ====\n",
    "optimizer_dict = {\n",
    "    i: torch.optim.Adam(list(encoders[i].parameters()) +\n",
    "                        [p for P in P_maps[i].values() for p in P.parameters()] +\n",
    "                        [p for Q in Q_maps[i].values() for p in Q.parameters()],\n",
    "                        lr=1e-3)\n",
    "    for i in range(num_nodes)\n",
    "}\n",
    "\n",
    "print(\"Dataset and graph setup complete.\")\n",
    "print(f\"Graph: {graph}\")\n",
    "print(f\"DataLoaders: {len(data_loaders)} nodes ready.\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c334b-6c76-4154-9344-5bdb2eee8586",
   "metadata": {},
   "source": [
    "# implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926e6e44-2e63-4574-8a9f-644fb634e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def sheaf_laplacian_loss(h_i, h_j, P_ij, P_ji):\n",
    "    \"\"\"\n",
    "    ||P_ij h_i - P_ji h_j||^2\n",
    "    \"\"\"\n",
    "    z_i = P_ij(h_i)\n",
    "    z_j = P_ji(h_j)\n",
    "    return F.mse_loss(z_i, z_j)\n",
    "\n",
    "\n",
    "# --- One Training Step ---\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=1.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    One node-centric sheaf training step\n",
    "    \"\"\"\n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        lap_loss += sheaf_laplacian_loss(\n",
    "            h_i, h_j, P_maps[node_id][j], P_maps[j][node_id]\n",
    "        )\n",
    "\n",
    "        # Transport embedding from j->i\n",
    "        transported = Q_maps[node_id][j](\n",
    "            P_maps[j][node_id](h_j)\n",
    "        )\n",
    "        \n",
    "        # Contrastive term\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "    \n",
    "    return lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for i in encoders:\n",
    "        encoders[i].to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "\n",
    "        # zip(*...) aligns batches across all nodes\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Compute local losses for each node\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                x_i = batch[0].to(device)  # Assume (data, label)\n",
    "\n",
    "                # Collect neighbors\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, x_i, neighbors_data, \n",
    "                    encoders[i], P_maps, Q_maps\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "                \n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: avg loss {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffddd166-c07f-4231-9424-40bcd491f3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: avg loss 7810.8638\n",
      "Epoch 2/20: avg loss 1115448.9068\n",
      "Epoch 3/20: avg loss 14653.7397\n",
      "Epoch 4/20: avg loss 9396.7694\n",
      "Epoch 5/20: avg loss 8551.4461\n",
      "Epoch 6/20: avg loss 8204.1638\n",
      "Epoch 7/20: avg loss 8030.7293\n",
      "Epoch 8/20: avg loss 7938.4958\n",
      "Epoch 9/20: avg loss 7889.1106\n",
      "Epoch 10/20: avg loss 8665.3208\n",
      "Epoch 11/20: avg loss 8380.7364\n",
      "Epoch 12/20: avg loss 8152.2225\n",
      "Epoch 13/20: avg loss 38808064.6614\n",
      "Epoch 14/20: avg loss 1838120.7026\n",
      "Epoch 15/20: avg loss 314059.2380\n",
      "Epoch 16/20: avg loss 96729.5935\n",
      "Epoch 17/20: avg loss 42831.1479\n",
      "Epoch 18/20: avg loss 23249.9032\n",
      "Epoch 19/20: avg loss 15309.3741\n",
      "Epoch 20/20: avg loss 11864.2920\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sheaf_node_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m train_sheaf_decentralized(\n\u001b[0;32m      2\u001b[0m     graph, data_loaders, encoders, P_maps, Q_maps,\n\u001b[0;32m      3\u001b[0m     optimizer_dict, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m \u001b[43msheaf_node_eval\u001b[49m(\n\u001b[0;32m      7\u001b[0m     graph, data_loaders, encoders, P_maps, Q_maps, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sheaf_node_eval' is not defined"
     ]
    }
   ],
   "source": [
    "train_sheaf_decentralized(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "    optimizer_dict, epochs=20, device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6713a16-dbcb-498e-a65d-3e5c161c13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Zero-Shot Classification]\n",
      "Node 0: best zero-shot acc 0.1044\n",
      "Node 1: best zero-shot acc 0.0987\n",
      "Node 2: best zero-shot acc 0.0987\n",
      "Average Zero-Shot Accuracy: 0.1006\n",
      "\n",
      "[Cross-Modal Retrieval]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_node(node_id, data_loader, encoder, device=\"cuda\"):\n",
    "    encoder.eval()\n",
    "    all_embs, all_labels = [], []\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        h = encoder(x)  # [B, d]\n",
    "        all_embs.append(h)\n",
    "        all_labels.append(y.to(device))\n",
    "    return torch.cat(all_embs, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sheaf_node_eval(graph, data_loaders, encoders, P_maps, Q_maps, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates:\n",
    "    1. Zero-shot node-level classification\n",
    "    2. Cross-modal retrieval between nodes\n",
    "    using transported embeddings.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Encode all nodes ---\n",
    "    node_embs, node_labels = {}, {}\n",
    "    for i in graph.keys():\n",
    "        h, y = encode_node(i, data_loaders[i], encoders[i].to(device), device)\n",
    "        node_embs[i], node_labels[i] = h, y\n",
    "\n",
    "    # --- Step 2: Zero-shot classification ---\n",
    "    print(\"\\n[Zero-Shot Classification]\")\n",
    "    zs_accs = []\n",
    "    for target in graph.keys():\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for source in graph.keys():\n",
    "            if source == target:\n",
    "                continue\n",
    "\n",
    "            # Transport source embeddings into target space\n",
    "            transported = Q_maps[target][source](\n",
    "                P_maps[source][target](node_embs[source])\n",
    "            )\n",
    "\n",
    "            # Nearest neighbor in embedding space\n",
    "            sim = torch.matmul(\n",
    "                F.normalize(transported, dim=-1),\n",
    "                F.normalize(node_embs[target], dim=-1).T\n",
    "            )  # [N_source, N_target]\n",
    "\n",
    "            preds = node_labels[target][sim.argmax(dim=-1)]\n",
    "            acc = accuracy_score(node_labels[source].cpu(), preds.cpu())\n",
    "            best_acc = max(best_acc, acc)\n",
    "\n",
    "        zs_accs.append(best_acc)\n",
    "        print(f\"Node {target}: best zero-shot acc {best_acc:.4f}\")\n",
    "\n",
    "    print(f\"Average Zero-Shot Accuracy: {sum(zs_accs)/len(zs_accs):.4f}\")\n",
    "\n",
    "    # --- Step 3: Cross-modal retrieval ---\n",
    "    print(\"\\n[Cross-Modal Retrieval]\")\n",
    "    retrieval_scores = []\n",
    "    for i in graph.keys():\n",
    "        for j in graph[i]:\n",
    "            if i < j:  # avoid double-counting\n",
    "                # Transport j embeddings to i space\n",
    "                h_j2i = Q_maps[i][j](P_maps[j][i](node_embs[j]))\n",
    "\n",
    "                # Cosine similarities\n",
    "                sim = F.cosine_similarity(\n",
    "                    F.normalize(node_embs[i], dim=-1)[:, None, :],\n",
    "                    F.normalize(h_j2i, dim=-1)[None, :, :],\n",
    "                    dim=-1\n",
    "                )  # [N_i, N_j]\n",
    "\n",
    "                avg_sim = sim.max(dim=1)[0].mean().item()\n",
    "                retrieval_scores.append(avg_sim)\n",
    "                print(f\"Node {i}<->{j}: avg retrieval sim {avg_sim:.4f}\")\n",
    "\n",
    "    print(f\"Average Retrieval Score: {sum(retrieval_scores)/len(retrieval_scores):.4f}\")\n",
    "    return zs_accs, retrieval_scores\n",
    "\n",
    "sheaf_node_eval(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6db621-6b5e-404b-b9aa-60488ac6907b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60b6d7-6a5f-4d55-8778-3ca20a73f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=1.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    node_id: int\n",
    "    batch_x: [B, input_dim] for local node\n",
    "    neighbors_data: dict {j: (batch_x_j, encoder_j)}\n",
    "    P_maps, Q_maps: dict {(i,j): RestrictionMap or TransportMap}\n",
    "    \"\"\"\n",
    "    B = batch_x.size(0)\n",
    "    \n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "    \n",
    "    # --- Compute outgoing messages ---\n",
    "    z_outgoing = {}\n",
    "    for j, P_ij in P_maps[node_id].items():\n",
    "        z_outgoing[j] = P_ij(h_i)  # [B, d_edge]\n",
    "\n",
    "    # --- Receive neighbor messages and compute losses ---\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        z_i_to_j = P_maps[node_id][j](h_i)\n",
    "        z_j_to_i = P_maps[j][node_id](h_j)\n",
    "        lap_loss += F.mse_loss(z_i_to_j, z_j_to_i)\n",
    "\n",
    "        # Transported embedding: Q_ij(P_ji(h_j))\n",
    "        transported = Q_maps[node_id][j](z_j_to_i)\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "\n",
    "    total_loss = lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, \n",
    "                              lambda_lap=1.0, beta_contrast=1.0,\n",
    "                              device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Each batch_nodes is a tuple: (x_i, y_i), ..., one per node\n",
    "            batch_count += 1\n",
    "\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                batch_x = batch[0].to(device)  # assuming (x, y)\n",
    "\n",
    "                # Get neighbor batches\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j].to(device))\n",
    "\n",
    "                enc_i = encoders[i].to(device)\n",
    "                enc_i.train()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, batch_x, neighbors_data, enc_i, \n",
    "                    P_maps, Q_maps, \n",
    "                    lambda_lap, beta_contrast\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "\n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_epoch_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"[Epoch {epoch+1}] Avg loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d95340-2891-4b48-9f69-bba7b853e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sheaf_decentralized(\n",
    "    graph=your_graph_dict,\n",
    "    data_loaders=your_dataloaders,\n",
    "    encoders=your_node_encoders,\n",
    "    P_maps=your_restriction_maps,\n",
    "    Q_maps=your_transport_maps,\n",
    "    optimizer_dict=your_optimizer_dict,\n",
    "    epochs=20,\n",
    "    lambda_lap=1.0,\n",
    "    beta_contrast=1.0,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
