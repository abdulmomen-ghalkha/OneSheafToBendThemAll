{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1b9380-dd5f-4e8f-a7d7-e7db7c3b9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and graph setup complete.\n",
      "Graph: {0: [1, 2], 1: [0, 2], 2: [0, 1]}\n",
      "DataLoaders: 3 nodes ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "import random\n",
    "\n",
    "# ==== 1. Multi-modal MNIST Dataset ====\n",
    "class MultiModalMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1.view(-1), m2.view(-1), m3.view(-1)), label  # flatten each modality\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# --- Build training and testing sets ---\n",
    "train_dataset = MultiModalMNIST(train=True)\n",
    "test_dataset = MultiModalMNIST(train=False)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ==== 2. Graph Setup ====\n",
    "# We have 3 nodes (modality-specific learners)\n",
    "num_nodes = 3\n",
    "graph = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 2],\n",
    "    2: [0, 1]\n",
    "}\n",
    "\n",
    "# Node-specific DataLoaders (extracting single modality)\n",
    "def modality_dataloader(dataset, modality_idx):\n",
    "    class SingleModality(Dataset):\n",
    "        def __init__(self, base_dataset, idx):\n",
    "            self.base = base_dataset\n",
    "            self.idx = idx\n",
    "        def __len__(self):\n",
    "            return len(self.base)\n",
    "        def __getitem__(self, i):\n",
    "            Xs, y = self.base[i]\n",
    "            return Xs[self.idx], y\n",
    "    return DataLoader(SingleModality(dataset, modality_idx), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_loaders = {i: modality_dataloader(train_dataset, i) for i in range(num_nodes)}\n",
    "\n",
    "# ==== 3. Node Encoders and Maps ====\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Node Encoder ---\n",
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# --- Maps ---\n",
    "class RestrictionMap(nn.Module):  # Node -> Edge\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(node_dim, edge_dim, bias=False)\n",
    "    def forward(self, h):\n",
    "        return self.map(h)\n",
    "\n",
    "class TransportMap(nn.Module):  # Edge -> Node\n",
    "    def __init__(self, edge_dim, node_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(edge_dim, node_dim, bias=False)\n",
    "    def forward(self, z):\n",
    "        return self.map(z)\n",
    "\n",
    "# --- Initialize models ---\n",
    "input_dim = 28*28      # flattened MNIST\n",
    "embedding_dim = 64\n",
    "edge_dim = 32\n",
    "\n",
    "encoders = {i: NodeEncoder(input_dim, embedding_dim) for i in range(num_nodes)}\n",
    "\n",
    "# Initialize P and Q maps\n",
    "P_maps = {i: {} for i in range(num_nodes)}\n",
    "Q_maps = {i: {} for i in range(num_nodes)}\n",
    "for i in range(num_nodes):\n",
    "    for j in graph[i]:\n",
    "        P_maps[i][j] = RestrictionMap(embedding_dim, edge_dim)\n",
    "        Q_maps[i][j] = TransportMap(edge_dim, embedding_dim)\n",
    "\n",
    "# ==== 4. Optimizers ====\n",
    "optimizer_dict = {\n",
    "    i: torch.optim.Adam(list(encoders[i].parameters()) +\n",
    "                        [p for P in P_maps[i].values() for p in P.parameters()] +\n",
    "                        [p for Q in Q_maps[i].values() for p in Q.parameters()],\n",
    "                        lr=1e-3)\n",
    "    for i in range(num_nodes)\n",
    "}\n",
    "\n",
    "print(\"Dataset and graph setup complete.\")\n",
    "print(f\"Graph: {graph}\")\n",
    "print(f\"DataLoaders: {len(data_loaders)} nodes ready.\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c334b-6c76-4154-9344-5bdb2eee8586",
   "metadata": {},
   "source": [
    "# implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926e6e44-2e63-4574-8a9f-644fb634e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def sheaf_laplacian_loss(h_i, h_j, P_ij, P_ji):\n",
    "    \"\"\"\n",
    "    ||P_ij h_i - P_ji h_j||^2\n",
    "    \"\"\"\n",
    "    z_i = P_ij(h_i)\n",
    "    z_j = P_ji(h_j)\n",
    "    return F.mse_loss(z_i, z_j)\n",
    "\n",
    "\n",
    "# --- One Training Step ---\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=1.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    One node-centric sheaf training step\n",
    "    \"\"\"\n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        lap_loss += sheaf_laplacian_loss(\n",
    "            h_i, h_j, P_maps[node_id][j], P_maps[j][node_id]\n",
    "        )\n",
    "\n",
    "        # Transport embedding from j->i\n",
    "        transported = Q_maps[node_id][j](\n",
    "            P_maps[j][node_id](h_j)\n",
    "        )\n",
    "        \n",
    "        # Contrastive term\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "    \n",
    "    return lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for i in encoders:\n",
    "        encoders[i].to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "\n",
    "        # zip(*...) aligns batches across all nodes\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Compute local losses for each node\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                x_i = batch[0].to(device)  # Assume (data, label)\n",
    "\n",
    "                # Collect neighbors\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, x_i, neighbors_data, \n",
    "                    encoders[i], P_maps, Q_maps\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "                \n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: avg loss {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddd166-c07f-4231-9424-40bcd491f3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: avg loss 7810.8638\n",
      "Epoch 2/20: avg loss 1115448.9068\n",
      "Epoch 3/20: avg loss 14653.7397\n",
      "Epoch 4/20: avg loss 9396.7694\n",
      "Epoch 5/20: avg loss 8551.4461\n",
      "Epoch 6/20: avg loss 8204.1638\n",
      "Epoch 7/20: avg loss 8030.7293\n"
     ]
    }
   ],
   "source": [
    "train_sheaf_decentralized(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "    optimizer_dict, epochs=20, device=device\n",
    ")\n",
    "\n",
    "sheaf_node_eval(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6db621-6b5e-404b-b9aa-60488ac6907b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60b6d7-6a5f-4d55-8778-3ca20a73f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=1.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    node_id: int\n",
    "    batch_x: [B, input_dim] for local node\n",
    "    neighbors_data: dict {j: (batch_x_j, encoder_j)}\n",
    "    P_maps, Q_maps: dict {(i,j): RestrictionMap or TransportMap}\n",
    "    \"\"\"\n",
    "    B = batch_x.size(0)\n",
    "    \n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "    \n",
    "    # --- Compute outgoing messages ---\n",
    "    z_outgoing = {}\n",
    "    for j, P_ij in P_maps[node_id].items():\n",
    "        z_outgoing[j] = P_ij(h_i)  # [B, d_edge]\n",
    "\n",
    "    # --- Receive neighbor messages and compute losses ---\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        z_i_to_j = P_maps[node_id][j](h_i)\n",
    "        z_j_to_i = P_maps[j][node_id](h_j)\n",
    "        lap_loss += F.mse_loss(z_i_to_j, z_j_to_i)\n",
    "\n",
    "        # Transported embedding: Q_ij(P_ji(h_j))\n",
    "        transported = Q_maps[node_id][j](z_j_to_i)\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "\n",
    "    total_loss = lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, \n",
    "                              lambda_lap=1.0, beta_contrast=1.0,\n",
    "                              device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Each batch_nodes is a tuple: (x_i, y_i), ..., one per node\n",
    "            batch_count += 1\n",
    "\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                batch_x = batch[0].to(device)  # assuming (x, y)\n",
    "\n",
    "                # Get neighbor batches\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j].to(device))\n",
    "\n",
    "                enc_i = encoders[i].to(device)\n",
    "                enc_i.train()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, batch_x, neighbors_data, enc_i, \n",
    "                    P_maps, Q_maps, \n",
    "                    lambda_lap, beta_contrast\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "\n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_epoch_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"[Epoch {epoch+1}] Avg loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d95340-2891-4b48-9f69-bba7b853e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sheaf_decentralized(\n",
    "    graph=your_graph_dict,\n",
    "    data_loaders=your_dataloaders,\n",
    "    encoders=your_node_encoders,\n",
    "    P_maps=your_restriction_maps,\n",
    "    Q_maps=your_transport_maps,\n",
    "    optimizer_dict=your_optimizer_dict,\n",
    "    epochs=20,\n",
    "    lambda_lap=1.0,\n",
    "    beta_contrast=1.0,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
