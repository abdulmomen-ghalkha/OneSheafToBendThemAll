{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1b9380-dd5f-4e8f-a7d7-e7db7c3b9bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and graph setup complete.\n",
      "Graph: {0: [1, 2], 1: [0, 2], 2: [0, 1]}\n",
      "DataLoaders: 3 nodes ready.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import ImageFilter\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "\n",
    "# ==== 1. Multi-modal MNIST Dataset ====\n",
    "class MultiModalMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1.view(-1), m2.view(-1), m3.view(-1)), label  # flatten each modality\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# ====== 1. Multi-modal MNIST dataset ======\n",
    "class MultiModalMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = datasets.MNIST(root=\"./data\", train=train, download=True)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        \n",
    "        # Modality 1: original\n",
    "        m1 = TF.to_tensor(img)\n",
    "        # Modality 2: edge-detected\n",
    "        m2 = TF.to_tensor(img.filter(ImageFilter.FIND_EDGES))\n",
    "        # Modality 3: inverted\n",
    "        m3 = TF.to_tensor(TF.invert(img))\n",
    "        \n",
    "        return (m1, m2, m3), label\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Build training and testing sets ---\n",
    "train_dataset = MultiModalMNIST(train=True)\n",
    "test_dataset = MultiModalMNIST(train=False)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ==== 2. Graph Setup ====\n",
    "# We have 3 nodes (modality-specific learners)\n",
    "num_nodes = 3\n",
    "graph = {\n",
    "    0: [1, 2],\n",
    "    1: [0, 2],\n",
    "    2: [0, 1]\n",
    "}\n",
    "\n",
    "# Node-specific DataLoaders (extracting single modality)\n",
    "def modality_dataloader(dataset, modality_idx):\n",
    "    class SingleModality(Dataset):\n",
    "        def __init__(self, base_dataset, idx):\n",
    "            self.base = base_dataset\n",
    "            self.idx = idx\n",
    "        def __len__(self):\n",
    "            return len(self.base)\n",
    "        def __getitem__(self, i):\n",
    "            Xs, y = self.base[i]\n",
    "            return Xs[self.idx], y\n",
    "    return DataLoader(SingleModality(dataset, modality_idx), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "data_loaders = {i: modality_dataloader(train_dataset, i) for i in range(num_nodes)}\n",
    "\n",
    "# ==== 3. Node Encoders and Maps ====\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Node Encoder ---\n",
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class TinyEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(16*7*7, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Maps ---\n",
    "class RestrictionMap(nn.Module):  # Node -> Edge\n",
    "    def __init__(self, node_dim, edge_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(node_dim, edge_dim, bias=False)\n",
    "    def forward(self, h):\n",
    "        return self.map(h)\n",
    "\n",
    "class TransportMap(nn.Module):  # Edge -> Node\n",
    "    def __init__(self, edge_dim, node_dim):\n",
    "        super().__init__()\n",
    "        self.map = nn.Linear(edge_dim, node_dim, bias=False)\n",
    "    def forward(self, z):\n",
    "        return self.map(z)\n",
    "\n",
    "# --- Initialize models ---\n",
    "input_dim = 28*28      # flattened MNIST\n",
    "embedding_dim = 64\n",
    "edge_dim = 64\n",
    "\n",
    "#encoders = {i: NodeEncoder(input_dim, embedding_dim) for i in range(num_nodes)}\n",
    "encoders = {i: TinyEncoder(embedding_dim).to(device) for i in range(num_nodes)}\n",
    "\n",
    "\n",
    "# Initialize P and Q maps\n",
    "P_maps = {i: {} for i in range(num_nodes)}\n",
    "Q_maps = {i: {} for i in range(num_nodes)}\n",
    "for i in range(num_nodes):\n",
    "    for j in graph[i]:\n",
    "        P_maps[i][j] = RestrictionMap(embedding_dim, edge_dim).to(device)\n",
    "        Q_maps[i][j] = TransportMap(edge_dim, embedding_dim).to(device)\n",
    "\n",
    "# ==== 4. Optimizers ====\n",
    "optimizer_dict = {\n",
    "    i: torch.optim.Adam(list(encoders[i].parameters()) +\n",
    "                        [p for P in P_maps[i].values() for p in P.parameters()] +\n",
    "                        [p for Q in Q_maps[i].values() for p in Q.parameters()],\n",
    "                        lr=1e-3)\n",
    "    for i in range(num_nodes)\n",
    "}\n",
    "\n",
    "print(\"Dataset and graph setup complete.\")\n",
    "print(f\"Graph: {graph}\")\n",
    "print(f\"DataLoaders: {len(data_loaders)} nodes ready.\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c334b-6c76-4154-9344-5bdb2eee8586",
   "metadata": {},
   "source": [
    "# implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926e6e44-2e63-4574-8a9f-644fb634e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def sheaf_laplacian_loss(h_i, h_j, P_ij, P_ji):\n",
    "    \"\"\"\n",
    "    ||P_ij h_i - P_ji h_j||^2\n",
    "    \"\"\"\n",
    "    z_i = P_ij(h_i)\n",
    "    z_j = P_ji(h_j)\n",
    "    return F.mse_loss(z_i, z_j)\n",
    "\n",
    "\n",
    "# --- One Training Step ---\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=10.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    One node-centric sheaf training step\n",
    "    \"\"\"\n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        lap_loss += sheaf_laplacian_loss(\n",
    "            h_i, h_j, P_maps[node_id][j], P_maps[j][node_id]\n",
    "        )\n",
    "\n",
    "        # Transport embedding from j->i\n",
    "        transported = Q_maps[node_id][j](\n",
    "            P_maps[j][node_id](h_j)\n",
    "        )\n",
    "        \n",
    "        # Contrastive term\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "        #print(lap_loss, contrast_loss, lambda_lap, beta_contrast)\n",
    "    \n",
    "    return lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for i in encoders:\n",
    "        encoders[i].to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "\n",
    "        # zip(*...) aligns batches across all nodes\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Compute local losses for each node\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                x_i = batch[0].to(device)  # Assume (data, label)\n",
    "\n",
    "                # Collect neighbors\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j])\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, x_i, neighbors_data, \n",
    "                    encoders[i], P_maps, Q_maps\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "                \n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: avg loss {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_node(node_id, data_loader, encoder, device=\"cuda\"):\n",
    "    encoder.eval()\n",
    "    all_embs, all_labels = [], []\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        h = encoder(x)  # [B, d]\n",
    "        all_embs.append(h)\n",
    "        all_labels.append(y.to(device))\n",
    "    return torch.cat(all_embs, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sheaf_node_eval(graph, data_loaders, encoders, P_maps, Q_maps, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates:\n",
    "    1. Zero-shot node-level classification\n",
    "    2. Cross-modal retrieval between nodes\n",
    "    using transported embeddings.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Encode all nodes ---\n",
    "    node_embs, node_labels = {}, {}\n",
    "    for i in graph.keys():\n",
    "        h, y = encode_node(i, data_loaders[i], encoders[i].to(device), device)\n",
    "        node_embs[i], node_labels[i] = h, y\n",
    "\n",
    "    # --- Step 2: Zero-shot classification ---\n",
    "    print(\"\\n[Zero-Shot Classification]\")\n",
    "    zs_accs = []\n",
    "    for target in graph.keys():\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for source in graph.keys():\n",
    "            if source == target:\n",
    "                continue\n",
    "\n",
    "            # Transport source embeddings into target space\n",
    "            transported = Q_maps[target][source](\n",
    "                P_maps[source][target](node_embs[source])\n",
    "            )\n",
    "\n",
    "            # Nearest neighbor in embedding space\n",
    "            sim = torch.matmul(\n",
    "                F.normalize(transported, dim=-1),\n",
    "                F.normalize(node_embs[target], dim=-1).T\n",
    "            )  # [N_source, N_target]\n",
    "\n",
    "            preds = node_labels[target][sim.argmax(dim=-1)]\n",
    "            acc = accuracy_score(node_labels[source].cpu(), preds.cpu())\n",
    "            best_acc = max(best_acc, acc)\n",
    "\n",
    "        zs_accs.append(best_acc)\n",
    "        print(f\"Node {target}: best zero-shot acc {best_acc:.4f}\")\n",
    "\n",
    "    print(f\"Average Zero-Shot Accuracy: {sum(zs_accs)/len(zs_accs):.4f}\")\n",
    "\n",
    "    # --- Step 3: Cross-modal retrieval ---\n",
    "    print(\"\\n[Cross-Modal Retrieval]\")\n",
    "    retrieval_scores = []\n",
    "    for i in graph.keys():\n",
    "        for j in graph[i]:\n",
    "            if i < j:  # avoid double-counting\n",
    "                # Transport j embeddings to i space\n",
    "                h_j2i = Q_maps[i][j](P_maps[j][i](node_embs[j]))\n",
    "\n",
    "                # Cosine similarities\n",
    "                sim = F.cosine_similarity(\n",
    "                    F.normalize(node_embs[i], dim=-1)[:, None, :],\n",
    "                    F.normalize(h_j2i, dim=-1)[None, :, :],\n",
    "                    dim=-1\n",
    "                )  # [N_i, N_j]\n",
    "\n",
    "                avg_sim = sim.max(dim=1)[0].mean().item()\n",
    "                retrieval_scores.append(avg_sim)\n",
    "                print(f\"Node {i}<->{j}: avg retrieval sim {avg_sim:.4f}\")\n",
    "\n",
    "    print(f\"Average Retrieval Score: {sum(retrieval_scores)/len(retrieval_scores):.4f}\")\n",
    "    return zs_accs, retrieval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffddd166-c07f-4231-9424-40bcd491f3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: avg loss 1716472630.9152\n",
      "Epoch 2/50: avg loss 88858493.9486\n",
      "Epoch 3/50: avg loss 26924558.3197\n",
      "Epoch 4/50: avg loss 11809284.0068\n",
      "Epoch 5/50: avg loss 5737292.7255\n",
      "Epoch 6/50: avg loss 2847134.6883\n",
      "Epoch 7/50: avg loss 1407418.8653\n",
      "Epoch 8/50: avg loss 669632.2762\n",
      "Epoch 9/50: avg loss 323574.0210\n",
      "Epoch 10/50: avg loss 167954.9782\n",
      "Epoch 11/50: avg loss 91574.4375\n",
      "Epoch 12/50: avg loss 971555652148.8871\n",
      "Epoch 13/50: avg loss 56850947120.3333\n",
      "Epoch 14/50: avg loss 26597788294.2500\n",
      "Epoch 15/50: avg loss 14189575567.7917\n",
      "Epoch 16/50: avg loss 7149780888.8125\n",
      "Epoch 17/50: avg loss 3333478882.3333\n",
      "Epoch 18/50: avg loss 1405959273.1094\n",
      "Epoch 19/50: avg loss 649293600.5208\n",
      "Epoch 20/50: avg loss 300215410.8932\n",
      "Epoch 21/50: avg loss 155136897.3477\n",
      "Epoch 22/50: avg loss 83462028.1169\n",
      "Epoch 23/50: avg loss 46180112.6489\n",
      "Epoch 24/50: avg loss 26081503.4200\n",
      "Epoch 25/50: avg loss 15078183.7037\n",
      "Epoch 26/50: avg loss 8548683.0741\n",
      "Epoch 27/50: avg loss 14416760382303.3047\n",
      "Epoch 28/50: avg loss 312126005144.0000\n",
      "Epoch 29/50: avg loss 99884229358.6667\n",
      "Epoch 30/50: avg loss 61039738917.3333\n",
      "Epoch 31/50: avg loss 40566911608.0000\n",
      "Epoch 32/50: avg loss 22783807324.6667\n",
      "Epoch 33/50: avg loss 11579976326.0000\n",
      "Epoch 34/50: avg loss 6034250845.7083\n",
      "Epoch 35/50: avg loss 3052022058.4375\n",
      "Epoch 36/50: avg loss 1504598553.1354\n",
      "Epoch 37/50: avg loss 741538021.6458\n",
      "Epoch 38/50: avg loss 386336494.1901\n",
      "Epoch 39/50: avg loss 216853989.9727\n",
      "Epoch 40/50: avg loss 132181530.1055\n",
      "Epoch 41/50: avg loss 88247142.4648\n",
      "Epoch 42/50: avg loss 31013895985190.2188\n",
      "Epoch 43/50: avg loss 1256723543893.3333\n",
      "Epoch 44/50: avg loss 266090134792.0000\n",
      "Epoch 45/50: avg loss 142425253989.3333\n",
      "Epoch 46/50: avg loss 92146265384.0000\n",
      "Epoch 47/50: avg loss 56084816820.6667\n",
      "Epoch 48/50: avg loss 29778468591.3333\n",
      "Epoch 49/50: avg loss 15158526742.8333\n",
      "Epoch 50/50: avg loss 7694794554.1667\n"
     ]
    }
   ],
   "source": [
    "train_sheaf_decentralized(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "    optimizer_dict, epochs=50, device=device\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26292899-db03-41d1-95da-284dc91488c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Zero-Shot Classification]\n",
      "Node 0: best zero-shot acc 0.1143\n",
      "Node 1: best zero-shot acc 0.1062\n",
      "Node 2: best zero-shot acc 0.0992\n",
      "Average Zero-Shot Accuracy: 0.1066\n",
      "\n",
      "[Cross-Modal Retrieval]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 13.41 GiB. GPU 0 has a total capacity of 31.74 GiB of which 4.37 GiB is free. Including non-PyTorch memory, this process has 27.37 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 13.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msheaf_node_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 167\u001b[0m, in \u001b[0;36msheaf_node_eval\u001b[0;34m(graph, data_loaders, encoders, P_maps, Q_maps, device)\u001b[0m\n\u001b[1;32m    164\u001b[0m h_j2i \u001b[38;5;241m=\u001b[39m Q_maps[i][j](P_maps[j][i](node_embs[j]))\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Cosine similarities\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_embs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_j2i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N_i, N_j]\u001b[39;00m\n\u001b[1;32m    173\u001b[0m avg_sim \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    174\u001b[0m retrieval_scores\u001b[38;5;241m.\u001b[39mappend(avg_sim)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.41 GiB. GPU 0 has a total capacity of 31.74 GiB of which 4.37 GiB is free. Including non-PyTorch memory, this process has 27.37 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 13.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "sheaf_node_eval(\n",
    "    graph, data_loaders, encoders, P_maps, Q_maps, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6db621-6b5e-404b-b9aa-60488ac6907b",
   "metadata": {},
   "source": [
    "# Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60b6d7-6a5f-4d55-8778-3ca20a73f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def decentralized_training_step(node_id, batch_x, \n",
    "                                neighbors_data, \n",
    "                                encoder, P_maps, Q_maps, \n",
    "                                lambda_lap=1.0, beta_contrast=1.0):\n",
    "    \"\"\"\n",
    "    node_id: int\n",
    "    batch_x: [B, input_dim] for local node\n",
    "    neighbors_data: dict {j: (batch_x_j, encoder_j)}\n",
    "    P_maps, Q_maps: dict {(i,j): RestrictionMap or TransportMap}\n",
    "    \"\"\"\n",
    "    B = batch_x.size(0)\n",
    "    \n",
    "    # --- Local embedding ---\n",
    "    h_i = encoder(batch_x)  # [B, d_node]\n",
    "    \n",
    "    # --- Compute outgoing messages ---\n",
    "    z_outgoing = {}\n",
    "    for j, P_ij in P_maps[node_id].items():\n",
    "        z_outgoing[j] = P_ij(h_i)  # [B, d_edge]\n",
    "\n",
    "    # --- Receive neighbor messages and compute losses ---\n",
    "    lap_loss, contrast_loss = 0.0, 0.0\n",
    "    for j, (x_j, enc_j) in neighbors_data.items():\n",
    "        h_j = enc_j(x_j)\n",
    "\n",
    "        # Laplacian term\n",
    "        z_i_to_j = P_maps[node_id][j](h_i)\n",
    "        z_j_to_i = P_maps[j][node_id](h_j)\n",
    "        lap_loss += F.mse_loss(z_i_to_j, z_j_to_i)\n",
    "\n",
    "        # Transported embedding: Q_ij(P_ji(h_j))\n",
    "        transported = Q_maps[node_id][j](z_j_to_i)\n",
    "        contrast_loss += contrastive_loss(h_i, transported)\n",
    "\n",
    "    total_loss = lambda_lap * lap_loss + beta_contrast * contrast_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def contrastive_loss(local_emb, transported_emb, temperature=0.1):\n",
    "    local_norm = F.normalize(local_emb, dim=-1)\n",
    "    transported_norm = F.normalize(transported_emb, dim=-1)\n",
    "    \n",
    "    logits = torch.matmul(local_norm, transported_norm.T) / temperature\n",
    "    labels = torch.arange(local_emb.size(0), device=local_emb.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def train_sheaf_decentralized(graph, data_loaders, encoders, P_maps, Q_maps,\n",
    "                              optimizer_dict, epochs=10, \n",
    "                              lambda_lap=1.0, beta_contrast=1.0,\n",
    "                              device=device):\n",
    "    \"\"\"\n",
    "    graph: adjacency dict {i: [j1, j2, ...]}\n",
    "    data_loaders: dict {i: DataLoader}\n",
    "    encoders: dict {i: NodeEncoder}\n",
    "    P_maps, Q_maps: dict {i: {j: map}}\n",
    "    optimizer_dict: dict {i: torch.optim.Optimizer}\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_losses = defaultdict(float)\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_nodes in zip(*data_loaders.values()):\n",
    "            # Each batch_nodes is a tuple: (x_i, y_i), ..., one per node\n",
    "            batch_count += 1\n",
    "\n",
    "            for i, batch in enumerate(batch_nodes):\n",
    "                batch_x = batch[0].to(device)  # assuming (x, y)\n",
    "\n",
    "                # Get neighbor batches\n",
    "                neighbors_data = {}\n",
    "                for j in graph[i]:\n",
    "                    x_j = batch_nodes[j][0].to(device)\n",
    "                    neighbors_data[j] = (x_j, encoders[j].to(device))\n",
    "\n",
    "                enc_i = encoders[i].to(device)\n",
    "                enc_i.train()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = decentralized_training_step(\n",
    "                    i, batch_x, neighbors_data, enc_i, \n",
    "                    P_maps, Q_maps, \n",
    "                    lambda_lap, beta_contrast\n",
    "                )\n",
    "\n",
    "                optimizer_dict[i].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_dict[i].step()\n",
    "\n",
    "                total_losses[i] += loss.item()\n",
    "\n",
    "        avg_epoch_loss = sum(total_losses.values()) / len(total_losses)\n",
    "        print(f\"[Epoch {epoch+1}] Avg loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d95340-2891-4b48-9f69-bba7b853e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "train_sheaf_decentralized(\n",
    "    graph=your_graph_dict,\n",
    "    data_loaders=your_dataloaders,\n",
    "    encoders=your_node_encoders,\n",
    "    P_maps=your_restriction_maps,\n",
    "    Q_maps=your_transport_maps,\n",
    "    optimizer_dict=your_optimizer_dict,\n",
    "    epochs=20,\n",
    "    lambda_lap=1.0,\n",
    "    beta_contrast=1.0,\n",
    "    device=device\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
